# Mini Data Pipeline com a API do GitHub

Este projeto implementa um pipeline de dados simplificado (Bronze, Silver, Gold) que busca dados de repositÃ³rios populares do GitHub, os processa, normaliza e gera mÃ©tricas analÃ­ticas.

## ğŸ¯ Objetivo do Projeto

O objetivo Ã© demonstrar a construÃ§Ã£o de um pipeline de dados robusto e modular, seguindo as melhores prÃ¡ticas de engenharia de dados, como a separaÃ§Ã£o em camadas (ingestÃ£o, normalizaÃ§Ã£o e analytics), tratamento de erros e capacidade de recuperaÃ§Ã£o.

## âœ¨ Principais Funcionalidades

* **Arquitetura em Camadas:** OrganizaÃ§Ã£o clara em Bronze (dados brutos), Silver (dados limpos) e Gold (mÃ©tricas de negÃ³cio).
* **IngestÃ£o Resiliente:** Utiliza um sistema de checkpoint para retomar a busca de dados de onde parou, evitando trabalho duplicado.
* **TolerÃ¢ncia a Falhas:** Implementa um mecanismo de *retry* com *backoff* exponencial para lidar com erros de API (como limites de taxa ou instabilidades do servidor).
* **EficiÃªncia de Armazenamento:** Salva os dados processados em formato Parquet, otimizado para performance e anÃ¡lise.
* **Modularidade:** O cÃ³digo Ã© estruturado em funÃ§Ãµes claras e distintas para cada etapa do pipeline.

## âš™ï¸ Como Funciona: Detalhes TÃ©cnicos

O pipeline Ã© orquestrado pelo script `main.py` e executa trÃªs estÃ¡gios principais:

### ğŸ¥‰ Camada Bronze (IngestÃ£o de Dados Brutos)

Nesta fase, o pipeline busca dados da API de busca de repositÃ³rios do GitHub.

* **Endpoint:** `https://api.github.com/search/repositories`
* **CritÃ©rios de Busca:** A consulta Ã© configurada para buscar repositÃ³rios com mais de 1000 estrelas (`stars:>1000`), ordenados pelo nÃºmero de estrelas em ordem decrescente.
* **Checkpoint:** O processo salva a Ãºltima pÃ¡gina buscada com sucesso. Se o pipeline for interrompido e executado novamente, ele continuarÃ¡ a partir da pÃ¡gina seguinte, garantindo que nenhum dado seja perdido e evitando reprocessamento desnecessÃ¡rio.
* **Tratamento de Erros:** O pipeline tentarÃ¡ novamente (com tempo de espera crescente) em caso de erros de servidor (`5xx`) ou de limite de taxa (`403`, `429`).
* **SaÃ­da:** Os dados brutos de cada pÃ¡gina sÃ£o salvos como arquivos JSON separados em `data/bronze/repositories/YYYY/MM/DD/page_{numero_da_pagina}.json`.

### ğŸ¥ˆ Camada Silver (Limpeza e NormalizaÃ§Ã£o)

A camada Silver transforma os dados brutos em um formato tabular, limpo e pronto para anÃ¡lise.

* **Leitura:** LÃª todos os arquivos JSON da camada Bronze.
* **DeduplicaÃ§Ã£o:** Remove registros duplicados com base no `id` do repositÃ³rio, mantendo a versÃ£o mais recente com base na data de `updated_at`.
* **NormalizaÃ§Ã£o:**
    * Expande o objeto aninhado `owner` em colunas separadas (ex: `owner_id`, `owner_login`).
    * Extrai a chave da licenÃ§a (`license.key`) para uma nova coluna `license_key`.
* **SaÃ­da:** A tabela limpa e normalizada Ã© salva como um Ãºnico arquivo Parquet em `data/silver/repositories/repositories.parquet`.

### ğŸ¥‡ Camada Gold (MÃ©tricas e AgregaÃ§Ã£o)

A camada final agrega os dados limpos para criar mÃ©tricas de negÃ³cio de alto nÃ­vel.

* **Leitura:** Carrega o arquivo Parquet da camada Silver.
* **MÃ©tricas Calculadas:**
    1.  **Novos RepositÃ³rios por Dia:** Contagem de quantos repositÃ³rios foram criados em cada data.
    2.  **MÃ©dia de Estrelas por Dia:** A mÃ©dia de "estrelas" (`stargazers_count`) dos repositÃ³rios, agrupada pelo dia de criaÃ§Ã£o.
    3.  **Ranking de Linguagens:** As 5 linguagens de programaÃ§Ã£o mais comuns entre os repositÃ³rios.
* **SaÃ­da:** As tabelas de mÃ©tricas sÃ£o salvas em formato Parquet no diretÃ³rio `data/gold/`.

### Por que usei o formato Parquet?

Neste projeto, os dados das camadas Silver e Gold sÃ£o salvos em formato **Parquet** em vez de formatos mais comuns como CSV ou JSON. A escolha foi intencional e baseia-se em duas vantagens principais para pipelines de dados:

1.  **EficiÃªncia e Performance:** O Parquet Ã© um formato de armazenamento colunar. Isso significa que, ao fazer uma consulta, o sistema pode ler apenas as colunas necessÃ¡rias para a operaÃ§Ã£o, ignorando as demais. No nosso caso, ao criar as mÃ©tricas da camada Gold, nÃ£o precisamos de todas as colunas da camada Silver. A leitura se torna muito mais rÃ¡pida e consome menos recursos computacionais.

2.  **Alta Taxa de CompressÃ£o:** Por agrupar dados do mesmo tipo (ex: todos os nÃºmeros da coluna "stars" ficam juntos), o Parquet consegue uma compressÃ£o muito mais eficaz. Os arquivos `.parquet` sÃ£o significativamente menores que seus equivalentes em `.csv` ou `.json`, o que economiza espaÃ§o de armazenamento e acelera a transferÃªncia de dados.

Em resumo, o Parquet Ã© o padrÃ£o da indÃºstria para dados analÃ­ticos, pois oferece um desempenho superior e maior eficiÃªncia de armazenamento, tornando o pipeline mais robusto e escalÃ¡vel.

## ğŸ“‚ Estrutura do Projeto

mini-pipeline-github/
â”œâ”€â”€ data/                 # DiretÃ³rio raiz para os dados (nÃ£o versionado)
â”‚   â”œâ”€â”€ bronze/           # Dados brutos da API
â”‚   â”œâ”€â”€ silver/           # Dados limpos e normalizados
â”‚   â”œâ”€â”€ gold/             # Tabelas analÃ­ticas e mÃ©tricas
â”‚   â””â”€â”€ checkpoints/      # Arquivos de estado para a ingestÃ£o
â”œâ”€â”€ run_pipeline.py       # Script principal com a lÃ³gica do pipeline
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto Python
â””â”€â”€ README.md             # Este arquivo

## ğŸš€ Como Executar

### PrÃ©-requisitos

* Python 3.8 ou superior

### Passos

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/gabrielabarbara1/pipeline_fh_data.git](https://github.com/gabrielabarbara1/pipeline_fh_data.git)
    cd pipeline_fh_data
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    # Cria o ambiente
    python -m venv venv

    # Ativa no Windows
    venv\Scripts\activate

    # Ativa no Linux/macOS
    source venv/bin/activate
    ```

3.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Execute o pipeline:**
    ```bash
    python main.py
    ```

ApÃ³s a execuÃ§Ã£o, os diretÃ³rios `data/bronze`, `data/silver` e `data/gold` estarÃ£o populados com os arquivos gerados em cada etapa. VocÃª poderÃ¡ ver o log de execuÃ§Ã£o no terminal, incluindo o ranking das 5 linguagens mais populares ao final.